{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e9bfadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/terezarafajova/Desktop/DTU leta fucking go/Deep learning/gnn_intro/.venv/lib/python3.13/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing torch-geometric...\n",
      "Installing wandb...\n",
      "All packages ready!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (if not already installed)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    'torch',\n",
    "    'torch-geometric',\n",
    "    'numpy',\n",
    "    'tqdm',\n",
    "    'wandb',\n",
    "]\n",
    "\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        __import__(pkg.replace('-', '_'))\n",
    "    except ImportError:\n",
    "        print(f'Installing {pkg}...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n",
    "\n",
    "print('All packages ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87e809e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/terezarafajova/Desktop/DTU leta fucking go/Deep learning/gnn_intro/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "print('Imports successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68315300",
   "metadata": {},
   "source": [
    "## GCN Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d6df55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN model defined\n"
     ]
    }
   ],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"Configurable GCN with optional batchnorm, dropout and mean+max pooled MLP readout.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_node_features,\n",
    "        hidden_channels: int = 128,\n",
    "        num_layers: int = 3,\n",
    "        dropout: float = 0.0,\n",
    "        use_batchnorm: bool = True,\n",
    "    ):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.dropout = float(dropout)\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "\n",
    "        # Build GCN layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        in_channels = num_node_features\n",
    "        for i in range(num_layers):\n",
    "            out_channels = hidden_channels\n",
    "            self.convs.append(GCNConv(in_channels, out_channels))\n",
    "            in_channels = out_channels\n",
    "\n",
    "        # Optional batchnorms\n",
    "        if use_batchnorm and num_layers > 0:\n",
    "            self.bns = nn.ModuleList([nn.BatchNorm1d(hidden_channels) for _ in range(num_layers)])\n",
    "        else:\n",
    "            self.bns = None\n",
    "\n",
    "        # MLP readout: (mean+max pooled) -> hidden -> 1\n",
    "        readout_hidden = max(hidden_channels // 2, 16)\n",
    "        self.readout = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_channels, readout_hidden),  # 2x because mean+max concat\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(readout_hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # Node embedding through stacked GCN layers\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            # Apply BatchNorm before the non-linearity\n",
    "            if self.bns is not None:\n",
    "                x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            if self.dropout > 0:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Readout: concatenate mean and max pooled node features per graph\n",
    "        mean_pool = global_mean_pool(x, batch)\n",
    "        max_pool = global_max_pool(x, batch)\n",
    "        x = torch.cat([mean_pool, max_pool], dim=1)  # [batch_size, 2*hidden_channels]\n",
    "\n",
    "        # MLP head -> returns shape [batch_size, 1]\n",
    "        out = self.readout(x)\n",
    "        return out\n",
    "\n",
    "print('GCN model defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5c82f0",
   "metadata": {},
   "source": [
    "## Configuration & Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d6fdf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Config: {'seed': 0, 'device': 'cpu', 'data_dir': './data', 'target_idx': 2, 'num_node_features': 11, 'hidden_channels': 128, 'num_layers': 3, 'dropout': 0.0, 'use_batchnorm': True, 'batch_size_train': 100, 'batch_size_inference': 2048, 'num_workers': 0, 'splits': [0.72, 0.08, 0.1, 0.1], 'subset_size': None, 'total_epochs': 250, 'validation_interval': 10, 'early_stopping_patience': 10, 'lr': 0.0005, 'weight_decay': 0.005, 'lambda_mt': 1.0, 'ema_decay': 0.99, 'mt_augment_scale': 0.05, 'grad_clip_norm': 0.0, 'use_target_normalization': True}\n"
     ]
    }
   ],
   "source": [
    "# Configuration (hardcoded)\n",
    "cfg = {\n",
    "    'seed': 0,\n",
    "    'device': 'cpu',  # use 'cuda' if GPU available\n",
    "    'data_dir': './data',\n",
    "    'target_idx': 2,  # QM9 property index (alpha polarizability)\n",
    "    \n",
    "    # Model\n",
    "    'num_node_features': 11,\n",
    "    'hidden_channels': 128,\n",
    "    'num_layers': 3,\n",
    "    'dropout': 0.0,\n",
    "    'use_batchnorm': True,\n",
    "    \n",
    "    # Dataset\n",
    "    'batch_size_train': 100,\n",
    "    'batch_size_inference': 2048,\n",
    "    'num_workers': 0,\n",
    "    'splits': [0.72, 0.08, 0.1, 0.1],\n",
    "    'subset_size': None,  # None = use full dataset\n",
    "    \n",
    "    # Trainer\n",
    "    'total_epochs': 250,\n",
    "    'validation_interval': 10,\n",
    "    'early_stopping_patience': 10,\n",
    "    'lr': 0.0005,\n",
    "    'weight_decay': 0.005,\n",
    "    \n",
    "    # Mean-Teacher\n",
    "    'lambda_mt': 1.0,\n",
    "    'ema_decay': 0.99,\n",
    "    'mt_augment_scale': 0.05,\n",
    "    'grad_clip_norm': 0.0,\n",
    "    'use_target_normalization': True,\n",
    "}\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(cfg['seed'])\n",
    "np.random.seed(cfg['seed'])\n",
    "\n",
    "device = torch.device(cfg['device'])\n",
    "print(f'Device: {device}')\n",
    "print(f'Config: {cfg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "711a7d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading QM9 dataset...\n",
      "Dataset size: 130831\n",
      "Node features: 11\n",
      "Edge features: 4\n",
      "Dataset size: 130831\n",
      "Node features: 11\n",
      "Edge features: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dp/rb5_3yk54ps7_zqq46yp65f00000gn/T/ipykernel_41302/586499891.py:6: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  if hasattr(dataset.data, 'edge_attr') and dataset.data.edge_attr is not None:\n",
      "/var/folders/dp/rb5_3yk54ps7_zqq46yp65f00000gn/T/ipykernel_41302/586499891.py:7: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  dataset.data.edge_attr = dataset.data.edge_attr.float()\n",
      "/var/folders/dp/rb5_3yk54ps7_zqq46yp65f00000gn/T/ipykernel_41302/586499891.py:10: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  y = dataset.data.y[:, cfg['target_idx']]\n",
      "/var/folders/dp/rb5_3yk54ps7_zqq46yp65f00000gn/T/ipykernel_41302/586499891.py:11: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  dataset.data.y = y\n"
     ]
    }
   ],
   "source": [
    "# Download and load QM9 dataset\n",
    "print('Downloading QM9 dataset...')\n",
    "dataset = QM9(root=cfg['data_dir'])\n",
    "\n",
    "# Normalize edge attributes if needed\n",
    "if hasattr(dataset.data, 'edge_attr') and dataset.data.edge_attr is not None:\n",
    "    dataset.data.edge_attr = dataset.data.edge_attr.float()\n",
    "\n",
    "# Get target property\n",
    "y = dataset.data.y[:, cfg['target_idx']]\n",
    "dataset.data.y = y\n",
    "\n",
    "print(f'Dataset size: {len(dataset)}')\n",
    "print(f'Node features: {dataset.num_node_features}')\n",
    "print(f'Edge features: {dataset.num_edge_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0e5f353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 94198, Val: 10466, Test: 26167, Unlabeled: 13084\n",
      "Dataloaders created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dp/rb5_3yk54ps7_zqq46yp65f00000gn/T/ipykernel_41302/3223541130.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  train_loader = DataLoader(train_data, batch_size=cfg['batch_size_train'], shuffle=True, num_workers=cfg['num_workers'])\n",
      "/var/folders/dp/rb5_3yk54ps7_zqq46yp65f00000gn/T/ipykernel_41302/3223541130.py:27: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  val_loader = DataLoader(val_data, batch_size=cfg['batch_size_inference'], shuffle=False, num_workers=cfg['num_workers'])\n",
      "/var/folders/dp/rb5_3yk54ps7_zqq46yp65f00000gn/T/ipykernel_41302/3223541130.py:28: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  test_loader = DataLoader(test_data, batch_size=cfg['batch_size_inference'], shuffle=False, num_workers=cfg['num_workers'])\n",
      "/var/folders/dp/rb5_3yk54ps7_zqq46yp65f00000gn/T/ipykernel_41302/3223541130.py:29: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  unlabeled_loader = DataLoader(unlabeled_data, batch_size=cfg['batch_size_train'], shuffle=True, num_workers=cfg['num_workers'])\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "n = len(dataset)\n",
    "splits = cfg['splits']\n",
    "indices = torch.randperm(n)\n",
    "\n",
    "train_idx = indices[:int(splits[0] * n)]\n",
    "val_idx = indices[int(splits[0] * n):int((splits[0] + splits[1]) * n)]\n",
    "test_idx = indices[int((splits[0] + splits[1]) * n):]\n",
    "\n",
    "# Split unlabeled (for semi-supervised)\n",
    "unlabeled_idx = indices[int((splits[0] + splits[1] + splits[2]) * n):]\n",
    "\n",
    "# Apply subset if configured\n",
    "if cfg['subset_size'] is not None:\n",
    "    train_idx = train_idx[:cfg['subset_size']]\n",
    "    unlabeled_idx = unlabeled_idx[:cfg['subset_size']]\n",
    "\n",
    "train_data = [dataset[i] for i in train_idx]\n",
    "val_data = [dataset[i] for i in val_idx]\n",
    "test_data = [dataset[i] for i in test_idx]\n",
    "unlabeled_data = [dataset[i] for i in unlabeled_idx]\n",
    "\n",
    "print(f'Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}, Unlabeled: {len(unlabeled_data)}')\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=cfg['batch_size_train'], shuffle=True, num_workers=cfg['num_workers'])\n",
    "val_loader = DataLoader(val_data, batch_size=cfg['batch_size_inference'], shuffle=False, num_workers=cfg['num_workers'])\n",
    "test_loader = DataLoader(test_data, batch_size=cfg['batch_size_inference'], shuffle=False, num_workers=cfg['num_workers'])\n",
    "unlabeled_loader = DataLoader(unlabeled_data, batch_size=cfg['batch_size_train'], shuffle=True, num_workers=cfg['num_workers'])\n",
    "\n",
    "print('Dataloaders created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b00c74f",
   "metadata": {},
   "source": [
    "## Trainer: Semi-Supervised Ensemble with Mean-Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f7fad03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SemiSupervisedEnsemble trainer defined\n"
     ]
    }
   ],
   "source": [
    "class SemiSupervisedEnsemble:\n",
    "    def __init__(\n",
    "        self,\n",
    "        supervised_criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        models,\n",
    "        logger,\n",
    "        datamodule,\n",
    "        lambda_mt,\n",
    "        ema_decay,\n",
    "        mt_augment_scale,\n",
    "        grad_clip_norm: float = 0.0,\n",
    "        use_target_normalization: bool = True,\n",
    "    ):\n",
    "        # Semi-supervised hyperparameters (configurable)\n",
    "        self.device = device\n",
    "        self.models = models\n",
    "        for m in self.models:\n",
    "            m.to(device)\n",
    "            for p in m.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "        self.teacher_models = deepcopy(models)\n",
    "        for teacher in self.teacher_models:\n",
    "            teacher.to(device)\n",
    "            for p in teacher.parameters():\n",
    "                p.requires_grad = False\n",
    "            # keep teacher in eval mode so it provides a stable target (no dropout/bn updates)\n",
    "            teacher.eval()\n",
    "\n",
    "        for m in self.models:\n",
    "            m.to(device)\n",
    "            for p in m.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "                \n",
    "        # set from init args so they can be configured via Hydra\n",
    "        self.lambda_mt = float(lambda_mt)\n",
    "        self.lambda_cps = 1.0\n",
    "        self.ema_decay = float(ema_decay)\n",
    "        self.mt_augment_scale = float(mt_augment_scale)\n",
    "        self.grad_clip_norm = float(grad_clip_norm)\n",
    "        self.use_target_normalization = bool(use_target_normalization)\n",
    "\n",
    "        # Optim related things\n",
    "        self.supervised_criterion = supervised_criterion\n",
    "        all_params = [p for m in self.models for p in m.parameters()]\n",
    "        self.optimizer = optimizer(params=all_params)\n",
    "        self.scheduler = scheduler(optimizer=self.optimizer)\n",
    "\n",
    "        # Dataloader setup\n",
    "        self.train_dataloader = datamodule.train_dataloader()\n",
    "        self.val_dataloader = datamodule.val_dataloader()\n",
    "        self.test_dataloader = datamodule.test_dataloader()\n",
    "        self.unlabeled_train_dataloader = datamodule.unsupervised_train_dataloader()\n",
    "\n",
    "        # Logging\n",
    "        self.logger = logger\n",
    "        # place to store best model weights found during training\n",
    "        self._best_state = None\n",
    "        self._best_epoch = None\n",
    "\n",
    "\n",
    "    # ---------------------------\n",
    "    # Mean Teacher EMA update\n",
    "    # ---------------------------\n",
    "    def update_teacher(self):\n",
    "        for teacher, student in zip(self.teacher_models, self.models):\n",
    "            for tp, sp in zip(teacher.parameters(), student.parameters()):\n",
    "                tp.data = self.ema_decay * tp.data + (1.0 - self.ema_decay) * sp.data\n",
    "\n",
    "    # ---------------------------\n",
    "    # N-CPS consistency\n",
    "    # ---------------------------\n",
    "    def noisy_augment(self, data):\n",
    "        # simple example: gaussian noise using configurable scale\n",
    "        noisy_x = data.x + float(self.mt_augment_scale) * torch.randn_like(data.x)\n",
    "        data_aug = deepcopy(data)\n",
    "        data_aug.x = noisy_x\n",
    "        return data_aug\n",
    "\n",
    "    def validate(self):\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, targets in self.val_dataloader:\n",
    "                x, targets = x.to(self.device), targets.to(self.device)\n",
    "                \n",
    "                # Ensemble prediction\n",
    "                preds = [model(x) for model in self.models]\n",
    "                # If using target normalization, model outputs are in normalized space;\n",
    "                # un-normalize before computing validation MSE so it's in original scale.\n",
    "                if getattr(self, 'use_target_normalization', False):\n",
    "                    preds = [(p * self.target_std + self.target_mean) for p in preds]\n",
    "                avg_preds = torch.stack(preds).mean(0)\n",
    "\n",
    "                val_loss = torch.nn.functional.mse_loss(avg_preds, targets)\n",
    "                val_losses.append(val_loss.item())\n",
    "        val_loss = np.mean(val_losses)\n",
    "        return {\"val_MSE\": val_loss}\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"Evaluate models on the test set and return test metrics.\n",
    "\n",
    "        Returns a dict like {\"test_MSE\": float}.\n",
    "        \"\"\"\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "\n",
    "        test_losses = []\n",
    "        with torch.no_grad():\n",
    "            for x, targets in self.test_dataloader:\n",
    "                x, targets = x.to(self.device), targets.to(self.device)\n",
    "\n",
    "                preds = [model(x) for model in self.models]\n",
    "                # If using target normalization, un-normalize predictions\n",
    "                if getattr(self, 'use_target_normalization', False):\n",
    "                    preds = [(p * self.target_std + self.target_mean) for p in preds]\n",
    "\n",
    "                avg_preds = torch.stack(preds).mean(0)\n",
    "                test_loss = torch.nn.functional.mse_loss(avg_preds, targets)\n",
    "                test_losses.append(test_loss.item())\n",
    "\n",
    "        test_loss = float(np.mean(test_losses)) if len(test_losses) > 0 else float('nan')\n",
    "        # log and return\n",
    "        try:\n",
    "            self.logger.log_dict({\"test_MSE\": test_loss})\n",
    "        except Exception:\n",
    "            pass\n",
    "        return {\"test_MSE\": test_loss}\n",
    "\n",
    "    def train(self, total_epochs, validation_interval, early_stopping_patience=None, **kwargs):\n",
    "        final_results = {}\n",
    "        patience_counter = 0\n",
    "        # allow overriding patience from config; default to 10 if not provided\n",
    "        patience = int(early_stopping_patience) if early_stopping_patience is not None else 10\n",
    "        best_val_loss = float('inf')\n",
    "\n",
    "        # If target normalization is enabled, compute train target mean/std once\n",
    "        self.target_mean = 0.0\n",
    "        self.target_std = 1.0\n",
    "        if self.use_target_normalization:\n",
    "            all_targets = []\n",
    "            for _, t in self.train_dataloader:\n",
    "                # t may be (batch,1) or (batch,)\n",
    "                if isinstance(t, (list, tuple)):\n",
    "                    t = t[0]\n",
    "                all_targets.append(t.detach().cpu())\n",
    "            if len(all_targets) > 0:\n",
    "                all_targets = torch.cat(all_targets, dim=0)\n",
    "                self.target_mean = float(all_targets.mean())\n",
    "                self.target_std = float(all_targets.std())\n",
    "                if self.target_std == 0:\n",
    "                    self.target_std = 1.0\n",
    "\n",
    "        unlabeled_iter = iter(self.unlabeled_train_dataloader)\n",
    "\n",
    "        for epoch in (pbar := tqdm(range(1, total_epochs + 1))):\n",
    "            for m in self.models:\n",
    "                m.train()\n",
    "\n",
    "            supervised_log = []\n",
    "            mt_log = []\n",
    "\n",
    "            for x_labeled, targets in self.train_dataloader:\n",
    "                # Get unlabeled batch\n",
    "                try:\n",
    "                    x_unl = next(unlabeled_iter)\n",
    "                except StopIteration:\n",
    "                    unlabeled_iter = iter(self.unlabeled_train_dataloader)\n",
    "                    x_unl = next(unlabeled_iter)\n",
    "\n",
    "                x_labeled, targets = x_labeled.to(self.device), targets.to(self.device)\n",
    "                x_unl = x_unl[0].to(self.device)\n",
    "\n",
    "                # create an augmented view for the student so MT loss is meaningful\n",
    "                x_unl_student = self.noisy_augment(x_unl)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # -------------------------\n",
    "                # 1. Supervised loss (optionally using target normalization)\n",
    "                # -------------------------\n",
    "                preds = [m(x_labeled) for m in self.models]\n",
    "\n",
    "                if self.use_target_normalization:\n",
    "                    targets_norm = (targets - self.target_mean) / self.target_std\n",
    "                else:\n",
    "                    targets_norm = targets\n",
    "\n",
    "                # loss used for backward (on normalized targets when enabled)\n",
    "                sup_losses = [self.supervised_criterion(p, targets_norm) for p in preds]\n",
    "                sup_loss = sum(sup_losses) / len(self.models)\n",
    "\n",
    "                # For logging, compute un-normalized MSE between ensemble preds and raw targets\n",
    "                try:\n",
    "                    with torch.no_grad():\n",
    "                        preds_un = [(p * self.target_std + self.target_mean) if self.use_target_normalization else p for p in preds]\n",
    "                        ensemble_un = torch.stack(preds_un).mean(0)\n",
    "                        supervised_log.append(torch.nn.functional.mse_loss(ensemble_un, targets).item())\n",
    "                except Exception:\n",
    "                    supervised_log.append(sup_loss.item())\n",
    "\n",
    "\n",
    "                # -------------------------\n",
    "                # 2. Mean Teacher loss\n",
    "                # -------------------------\n",
    "                # student sees augmented view, teacher sees original (stable) view\n",
    "                student_out = [m(x_unl_student) for m in self.models]\n",
    "                teacher_out = [tm(x_unl).detach() for tm in self.teacher_models]\n",
    "\n",
    "\n",
    "                mt_loss = 0\n",
    "                for s, t in zip(student_out, teacher_out):\n",
    "                    mt_loss += torch.nn.functional.mse_loss(s, t)\n",
    "                mt_loss = mt_loss / len(self.models)\n",
    "                mt_log.append(mt_loss.item())\n",
    "\n",
    "\n",
    "                # -------------------------\n",
    "                # Total loss\n",
    "                # -------------------------\n",
    "                loss = sup_loss + self.lambda_mt * mt_loss\n",
    "                loss.backward()\n",
    "\n",
    "                # gradient clipping (if enabled)\n",
    "                if self.grad_clip_norm and self.grad_clip_norm > 0.0:\n",
    "                    params = [p for m in self.models for p in m.parameters() if p.grad is not None]\n",
    "                    torch.nn.utils.clip_grad_norm_(params, self.grad_clip_norm)\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Update EMA teacher\n",
    "                self.update_teacher()\n",
    "\n",
    "            self.scheduler.step()\n",
    "\n",
    "            summary_dict = {\n",
    "                \"supervised_loss\": np.mean(supervised_log),\n",
    "                \"mean_teacher_loss\": np.mean(mt_log),\n",
    "            }\n",
    "\n",
    "            if epoch % validation_interval == 0 or epoch == total_epochs:\n",
    "                val_metrics = self.validate()\n",
    "                summary_dict.update(val_metrics)\n",
    "                pbar.set_postfix(summary_dict)\n",
    "\n",
    "                # Early stopping\n",
    "                cur_val = val_metrics[\"val_MSE\"]\n",
    "                if cur_val < best_val_loss:\n",
    "                    best_val_loss = cur_val\n",
    "                    patience_counter = 0\n",
    "                    # save best model weights (deepcopy state_dicts)\n",
    "                    try:\n",
    "                        self._best_state = [ {k: v.cpu().clone() for k, v in m.state_dict().items()} for m in self.models ]\n",
    "                        self._best_epoch = epoch\n",
    "                    except Exception:\n",
    "                        self._best_state = None\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"Early stopping at epoch {epoch}\")\n",
    "                        break\n",
    "\n",
    "            self.logger.log_dict(summary_dict, step=epoch)\n",
    "            final_results = summary_dict\n",
    "\n",
    "        # If we saved a best checkpoint during training, restore it so subsequent\n",
    "        # testing uses the best-validation weights rather than the final weights.\n",
    "        if self._best_state is not None:\n",
    "            try:\n",
    "                for m, state in zip(self.models, self._best_state):\n",
    "                    m.load_state_dict(state)\n",
    "                print(f\"Restored best model from epoch {self._best_epoch} for testing/evaluation.\")\n",
    "            except Exception:\n",
    "                print(\"Failed to restore best model state; using final weights.\")\n",
    "\n",
    "        return final_results\n",
    "\n",
    "print('SemiSupervisedEnsemble trainer defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab92c6b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "190f2da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and trainer ready. Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 4/250 [01:15<1:17:26, 18.89s/it]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     71\u001b[39m trainer = SemiSupervisedEnsemble(\n\u001b[32m     72\u001b[39m     supervised_criterion=criterion,\n\u001b[32m     73\u001b[39m     optimizer=optimizer_factory,\n\u001b[32m   (...)\u001b[39m\u001b[32m     83\u001b[39m     use_target_normalization=cfg[\u001b[33m'\u001b[39m\u001b[33muse_target_normalization\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     84\u001b[39m )\n\u001b[32m     86\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mModel and trainer ready. Starting training...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m train_results = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtotal_epochs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalidation_interval\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mearly_stopping_patience\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTraining complete. Final results: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_results\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 215\u001b[39m, in \u001b[36mSemiSupervisedEnsemble.train\u001b[39m\u001b[34m(self, total_epochs, validation_interval, early_stopping_patience, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[38;5;66;03m# 2. Mean Teacher loss\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# student sees augmented view, teacher sees original (stable) view\u001b[39;00m\n\u001b[32m    214\u001b[39m student_out = [m(x_unl_student) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.models]\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m teacher_out = [\u001b[43mtm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_unl\u001b[49m\u001b[43m)\u001b[49m.detach() \u001b[38;5;28;01mfor\u001b[39;00m tm \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.teacher_models]\n\u001b[32m    218\u001b[39m mt_loss = \u001b[32m0\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(student_out, teacher_out):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DTU leta fucking go/Deep learning/gnn_intro/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DTU leta fucking go/Deep learning/gnn_intro/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mGCN.forward\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Node embedding through stacked GCN layers\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.convs):\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     x = \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# Apply BatchNorm before the non-linearity\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DTU leta fucking go/Deep learning/gnn_intro/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DTU leta fucking go/Deep learning/gnn_intro/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Create wrapper to convert PyG Data objects to (x, targets) tuples\n",
    "class TupleDataLoader:\n",
    "    def __init__(self, loader, has_labels=True):\n",
    "        self.loader = loader\n",
    "        self.has_labels = has_labels\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for batch in self.loader:\n",
    "            if self.has_labels:\n",
    "                # Return (batch, targets) where targets is extracted from batch.y\n",
    "                targets = batch.y.view(-1, 1) if batch.y.dim() == 1 else batch.y\n",
    "                yield batch, targets\n",
    "            else:\n",
    "                # For unlabeled data, return (batch,) as a tuple\n",
    "                yield (batch,)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "\n",
    "# Create a simple datamodule wrapper\n",
    "class SimpleDataModule:\n",
    "    def __init__(self, train_loader, val_loader, test_loader, unlabeled_loader):\n",
    "        self._train_loader = TupleDataLoader(train_loader, has_labels=True)\n",
    "        self._val_loader = TupleDataLoader(val_loader, has_labels=True)\n",
    "        self._test_loader = TupleDataLoader(test_loader, has_labels=True)\n",
    "        self._unlabeled_loader = TupleDataLoader(unlabeled_loader, has_labels=False)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self._train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self._val_loader\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self._test_loader\n",
    "    \n",
    "    def unsupervised_train_dataloader(self):\n",
    "        return self._unlabeled_loader\n",
    "\n",
    "# Create a simple logger\n",
    "class SimpleLogger:\n",
    "    def log_dict(self, metrics, step=None):\n",
    "        # Just print for standalone notebook\n",
    "        pass\n",
    "\n",
    "# Create model\n",
    "model = GCN(\n",
    "    num_node_features=cfg['num_node_features'],\n",
    "    hidden_channels=cfg['hidden_channels'],\n",
    "    num_layers=cfg['num_layers'],\n",
    "    dropout=cfg['dropout'],\n",
    "    use_batchnorm=cfg['use_batchnorm'],\n",
    ")\n",
    "models = [model]\n",
    "\n",
    "# Loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Create datamodule and logger\n",
    "datamodule = SimpleDataModule(train_loader, val_loader, test_loader, unlabeled_loader)\n",
    "logger = SimpleLogger()\n",
    "\n",
    "# Optimizer and scheduler factories (trainer will instantiate them)\n",
    "def optimizer_factory(params):\n",
    "    return torch.optim.AdamW(params, lr=cfg['lr'], weight_decay=cfg['weight_decay'])\n",
    "\n",
    "def scheduler_factory(optimizer):\n",
    "    return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg['total_epochs'])\n",
    "\n",
    "# Create trainer\n",
    "trainer = SemiSupervisedEnsemble(\n",
    "    supervised_criterion=criterion,\n",
    "    optimizer=optimizer_factory,\n",
    "    scheduler=scheduler_factory,\n",
    "    device=device,\n",
    "    models=models,\n",
    "    logger=logger,\n",
    "    datamodule=datamodule,\n",
    "    lambda_mt=cfg['lambda_mt'],\n",
    "    ema_decay=cfg['ema_decay'],\n",
    "    mt_augment_scale=cfg['mt_augment_scale'],\n",
    "    grad_clip_norm=cfg['grad_clip_norm'],\n",
    "    use_target_normalization=cfg['use_target_normalization'],\n",
    ")\n",
    "\n",
    "print('Model and trainer ready. Starting training...')\n",
    "train_results = trainer.train(\n",
    "    total_epochs=cfg['total_epochs'],\n",
    "    validation_interval=cfg['validation_interval'],\n",
    "    early_stopping_patience=cfg['early_stopping_patience']\n",
    ")\n",
    "print(f'Training complete. Final results: {train_results}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8350ca40",
   "metadata": {},
   "source": [
    "## Testing & Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a340140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run test\n",
    "test_results = trainer.test()\n",
    "print(f'Test results: {test_results}')\n",
    "\n",
    "# Run final validation\n",
    "val_results = trainer.validate()\n",
    "print(f'Validation results: {val_results}')\n",
    "\n",
    "print(f'\\n=== Summary ===')\n",
    "print(f'Train (supervised + MT): {train_results}')\n",
    "print(f'Validation MSE: {val_results[\"val_MSE\"]:.6f}')\n",
    "print(f'Test MSE: {test_results[\"test_MSE\"]:.6f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
